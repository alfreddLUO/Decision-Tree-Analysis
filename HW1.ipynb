{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "6a3740ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n",
      "0               63.03        22.55                  39.61         40.48   \n",
      "1               39.06        10.06                  25.02         29.00   \n",
      "2               68.83        22.22                  50.09         46.61   \n",
      "3               69.30        24.65                  44.31         44.64   \n",
      "4               49.71         9.65                  28.32         40.06   \n",
      "..                ...          ...                    ...           ...   \n",
      "305             47.90        13.62                  36.00         34.29   \n",
      "306             53.94        20.72                  29.22         33.22   \n",
      "307             61.45        22.69                  46.17         38.75   \n",
      "308             45.25         8.69                  41.58         36.56   \n",
      "309             33.84         5.07                  36.64         28.77   \n",
      "\n",
      "     pelvic_radius  degree_spondylolisthesis class  \n",
      "0            98.67                     -0.25    DH  \n",
      "1           114.41                      4.56    DH  \n",
      "2           105.99                     -3.53    DH  \n",
      "3           101.87                     11.21    DH  \n",
      "4           108.17                      7.92    DH  \n",
      "..             ...                       ...   ...  \n",
      "305         117.45                     -4.25    NO  \n",
      "306         114.37                     -0.42    NO  \n",
      "307         125.67                     -2.71    NO  \n",
      "308         118.55                      0.21    NO  \n",
      "309         123.95                     -0.20    NO  \n",
      "\n",
      "[310 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "def read_data():\n",
    "    data = pd.read_table('./vertebral_column_data/column_3C.dat',header=None, sep=' ')\n",
    "    data.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius',\n",
    "                    'degree_spondylolisthesis', 'class']\n",
    "    return data\n",
    "\n",
    "def generate_data_info(data):\n",
    "    features_set = data.columns[:6]\n",
    "    label_name = data.columns[6]\n",
    "    training_labels = data.iloc[:,6]\n",
    "    overall_features_data = data.iloc[:, :6]\n",
    "    return features_set, label_name, training_labels, overall_features_data\n",
    "# features_set, label_name, training_labels, overall_features_data = generate_data_info(read_data())\n",
    "print(read_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "0209e5f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def train_split(test_size, shuffle, random_state):\n",
    "    data = read_data()\n",
    "    features_set, label_name, training_labels, overall_features_data = generate_data_info(data)\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(overall_features_data, training_labels,\n",
    "                                                                        test_size=test_size, shuffle=True,\n",
    "                                                                        random_state=random_state, \n",
    "                                                                        stratify=training_labels)\n",
    "    return data_train, data_test, labels_train, labels_test, features_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "0db26ca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "def predict(max_depth, criterion, data_train, labels_train, data_test, labels_test):\n",
    "    model = tree.DecisionTreeClassifier(max_depth=max_depth, criterion=criterion)\n",
    "    model = model.fit(data_train, labels_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "29928669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "def save_tree_file(model, test_size, random_state, max_depth, criterion, scores, features_set):\n",
    "    dot_data = tree.export_graphviz(model, out_file=None,\n",
    "        feature_names=features_set,\n",
    "        class_names=label_name,\n",
    "        filled=True, rounded=True,\n",
    "        special_characters=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graph.format='png'\n",
    "    graph.render('./Decision_Tree_GraphsGraph-TestSize({})-RandomState({})-MaxDepth({})-Criterion({})-Score({})'\n",
    "                 .format(test_size, random_state, max_depth, criterion, scores), view=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "62144a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_data(x_array, y_array, x_label, y_label, criterion,graph_name):\n",
    "    plt.title(graph_name)\n",
    "    plt.scatter(x_array, y_array, c=criterion)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.savefig('./Decision_Tree_Analysis_Graph/{}.jpg'.format(graph_name))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "95170362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max TestSize:0.1 State:27 Depth:6 Criterion: entropy model score: 0.9354838709677419 Node Count: 41 \n",
      " DH_Scores: 1.0 SL_Scores: 1.0 NO_Scores: 0.8 \n",
      " DH_SL_Scores: 1.0 DH_NO_Scores: 0.8 SL_NO_Scores: 0.8\n",
      "Max TestSize:0.1 State:27 Depth:4 Criterion: gini model score: 0.9032258064516129 Node Count: 25 \n",
      " DH_Scores: 0.6666666666666666 SL_Scores: 1.0 NO_Scores: 0.9 \n",
      " DH_SL_Scores: 1.0 DH_NO_Scores: 0.9 SL_NO_Scores: 0.9\n",
      "Max TestSize:0.2 State:27 Depth:6 Criterion: entropy model score: 0.9193548387096774 Node Count: 41 \n",
      " DH_Scores: 0.9166666666666666 SL_Scores: 1.0 NO_Scores: 0.8 \n",
      " DH_SL_Scores: 1.0 DH_NO_Scores: 0.8 SL_NO_Scores: 0.8\n",
      "Max TestSize:0.2 State:27 Depth:3 Criterion: gini model score: 0.9193548387096774 Node Count: 13 \n",
      " DH_Scores: 0.75 SL_Scores: 1.0 NO_Scores: 0.9 \n",
      " DH_SL_Scores: 1.0 DH_NO_Scores: 0.9 SL_NO_Scores: 0.9\n",
      "Max TestSize:0.30000000000000004 State:27 Depth:4 Criterion: entropy model score: 0.8936170212765957 Node Count: 21 \n",
      " DH_Scores: 0.7222222222222222 SL_Scores: 1.0 NO_Scores: 0.8333333333333334 \n",
      " DH_SL_Scores: 1.0 DH_NO_Scores: 0.8333333333333334 SL_NO_Scores: 0.8333333333333334\n",
      "Max TestSize:0.30000000000000004 State:27 Depth:3 Criterion: gini model score: 0.8829787234042553 Node Count: 15 \n",
      " DH_Scores: 0.8333333333333334 SL_Scores: 0.9782608695652174 NO_Scores: 0.7666666666666667 \n",
      " DH_SL_Scores: 0.9782608695652174 DH_NO_Scores: 0.7666666666666667 SL_NO_Scores: 0.7666666666666667\n",
      "Max TestSize:0.4 State:27 Depth:3 Criterion: entropy model score: 0.8629032258064516 Node Count: 13 \n",
      " DH_Scores: 0.6666666666666666 SL_Scores: 0.9666666666666667 NO_Scores: 0.825 \n",
      " DH_SL_Scores: 0.9666666666666667 DH_NO_Scores: 0.825 SL_NO_Scores: 0.825\n",
      "Max TestSize:0.4 State:27 Depth:4 Criterion: gini model score: 0.8709677419354839 Node Count: 21 \n",
      " DH_Scores: 0.6666666666666666 SL_Scores: 0.9666666666666667 NO_Scores: 0.85 \n",
      " DH_SL_Scores: 0.9666666666666667 DH_NO_Scores: 0.85 SL_NO_Scores: 0.85\n",
      "Max TestSize:0.5 State:43 Depth:9 Criterion: entropy model score: 0.8709677419354839 Node Count: 37 \n",
      " DH_Scores: 0.8333333333333334 SL_Scores: 0.9733333333333334 NO_Scores: 0.74 \n",
      " DH_SL_Scores: 0.9733333333333334 DH_NO_Scores: 0.74 SL_NO_Scores: 0.74\n",
      "Max TestSize:0.5 State:27 Depth:4 Criterion: gini model score: 0.832258064516129 Node Count: 17 \n",
      " DH_Scores: 0.7 SL_Scores: 0.96 NO_Scores: 0.72 \n",
      " DH_SL_Scores: 0.96 DH_NO_Scores: 0.72 SL_NO_Scores: 0.72\n",
      "Max TestSize:0.6 State:27 Depth:3 Criterion: entropy model score: 0.8387096774193549 Node Count: 13 \n",
      " DH_Scores: 0.6944444444444444 SL_Scores: 0.9666666666666667 NO_Scores: 0.7333333333333333 \n",
      " DH_SL_Scores: 0.9666666666666667 DH_NO_Scores: 0.7333333333333333 SL_NO_Scores: 0.7333333333333333\n",
      "Max TestSize:0.6 State:7 Depth:3 Criterion: gini model score: 0.8440860215053764 Node Count: 13 \n",
      " DH_Scores: 0.6666666666666666 SL_Scores: 0.9111111111111111 NO_Scores: 0.85 \n",
      " DH_SL_Scores: 0.9111111111111111 DH_NO_Scores: 0.85 SL_NO_Scores: 0.85\n",
      "Max TestSize:0.7000000000000001 State:43 Depth:4 Criterion: entropy model score: 0.8440366972477065 Node Count: 19 \n",
      " DH_Scores: 0.8095238095238095 SL_Scores: 0.9433962264150944 NO_Scores: 0.7142857142857143 \n",
      " DH_SL_Scores: 0.9433962264150944 DH_NO_Scores: 0.7142857142857143 SL_NO_Scores: 0.7142857142857143\n",
      "Max TestSize:0.7000000000000001 State:43 Depth:4 Criterion: gini model score: 0.8394495412844036 Node Count: 21 \n",
      " DH_Scores: 0.8095238095238095 SL_Scores: 0.9716981132075472 NO_Scores: 0.6571428571428571 \n",
      " DH_SL_Scores: 0.9716981132075472 DH_NO_Scores: 0.6571428571428571 SL_NO_Scores: 0.6571428571428571\n",
      "Max TestSize:0.8 State:27 Depth:8 Criterion: entropy model score: 0.7862903225806451 Node Count: 19 \n",
      " DH_Scores: 0.4375 SL_Scores: 0.9666666666666667 NO_Scores: 0.725 \n",
      " DH_SL_Scores: 0.9666666666666667 DH_NO_Scores: 0.725 SL_NO_Scores: 0.725\n",
      "Max TestSize:0.8 State:27 Depth:8 Criterion: gini model score: 0.8145161290322581 Node Count: 19 \n",
      " DH_Scores: 0.5208333333333334 SL_Scores: 0.9666666666666667 NO_Scores: 0.7625 \n",
      " DH_SL_Scores: 0.9666666666666667 DH_NO_Scores: 0.7625 SL_NO_Scores: 0.7625\n",
      "Max TestSize:0.9 State:43 Depth:3 Criterion: entropy model score: 0.8064516129032258 Node Count: 7 \n",
      " DH_Scores: 0.7037037037037037 SL_Scores: 0.9629629629629629 NO_Scores: 0.6333333333333333 \n",
      " DH_SL_Scores: 0.9629629629629629 DH_NO_Scores: 0.6333333333333333 SL_NO_Scores: 0.6333333333333333\n",
      "Max TestSize:0.9 State:43 Depth:3 Criterion: gini model score: 0.8064516129032258 Node Count: 7 \n",
      " DH_Scores: 0.7037037037037037 SL_Scores: 0.9629629629629629 NO_Scores: 0.6333333333333333 \n",
      " DH_SL_Scores: 0.9629629629629629 DH_NO_Scores: 0.6333333333333333 SL_NO_Scores: 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "def generate_train_split_decision_trees():\n",
    "    random_state_list = [7,27, 43]\n",
    "    criterion_list = ['entropy', 'gini']\n",
    "    features_set=''\n",
    "    max_scores_list = []\n",
    "    max_test_size_list = []\n",
    "    max_criterion_list = []\n",
    "    max_depth_list = []\n",
    "    max_node_count_list = []\n",
    "    DH_Scores_list = []\n",
    "    SL_Scores_list = []\n",
    "    NO_Scores_list = []\n",
    "    DH_SL_Scores_list = []\n",
    "    DH_NO_Scores_list = []\n",
    "    SL_NO_Scores_list = []\n",
    "\n",
    "    for testSize in np.arange(0.1, 1, 0.1):\n",
    "        for cri in criterion_list:\n",
    "            max_score = 0\n",
    "            max_test_size = 0\n",
    "            max_random_state = 0\n",
    "            max_depth = 0\n",
    "            max_criterion = ''\n",
    "            max_confusion_matrix= ''\n",
    "            max_tree_value = ''\n",
    "            max_decision_path = ''\n",
    "            max_model = ''\n",
    "            max_node_count = 0\n",
    "            DH_Scores_TMP = 0\n",
    "            SL_Scores_TMP = 0\n",
    "            NO_Scores_TMP = 0\n",
    "            DH_SL_Scores_TMP = 0\n",
    "            DH_NO_Scores_TMP = 0\n",
    "            SL_NO_Scores_TMP = 0\n",
    "            for state in random_state_list:\n",
    "                for depth in range(1, 10):\n",
    "                        data_train, data_test, labels_train, labels_test, features_set=train_split(testSize, \n",
    "                                                                                                   True, state)\n",
    "                        model = predict(depth, cri, data_train, labels_train, data_test, labels_test)\n",
    "                        labels_predict = model.predict(data_test)\n",
    "                        score = accuracy_score(labels_test, labels_predict)\n",
    "                        df = pd.concat([data_test, labels_test], axis=1)\n",
    "                        \n",
    "                        df_DH = df[df['class']=='DH']\n",
    "                        DH_data_test = df_DH.iloc[:, :6]\n",
    "                        DH_labels_test = df_DH.iloc[:, 6]\n",
    "                        DH_labels_predict = model.predict(DH_data_test)\n",
    "                        DH_Scores = accuracy_score(DH_labels_test, DH_labels_predict)\n",
    "                        \n",
    "                        df_SL = df[df['class']=='SL']\n",
    "                        SL_data_test = df_SL.iloc[:, :6]\n",
    "                        SL_labels_test = df_SL.iloc[:, 6]\n",
    "                        SL_labels_predict = model.predict(SL_data_test)\n",
    "                        SL_Scores = accuracy_score(SL_labels_test, SL_labels_predict)\n",
    "                        \n",
    "                        df_NO = df[df['class']=='NO']\n",
    "                        NO_data_test = df_NO.iloc[:, :6]\n",
    "                        NO_labels_test = df_NO.iloc[:, 6]\n",
    "                        NO_labels_predict = model.predict(NO_data_test)\n",
    "                        NO_Scores = accuracy_score(NO_labels_test, NO_labels_predict)\n",
    "                        \n",
    "                        df_DH_SL = df[df['class']==('SL' or 'DH')]\n",
    "                        DH_SL_data_test = df_DH_SL.iloc[:, :6]\n",
    "                        DH_SL_labels_test = df_DH_SL.iloc[:, 6]\n",
    "                        DH_SL_labels_test.replace('DH', 'SL')\n",
    "                        DH_SL_labels_predict = model.predict(DH_SL_data_test)\n",
    "                        DH_SL_labels_predict[1].replace('DH', 'SL')\n",
    "                        DH_SL_Scores = accuracy_score(DH_SL_labels_test, DH_SL_labels_predict)\n",
    "                        \n",
    "                        df_DH_NO = df[df['class']==('NO' or 'DH')]\n",
    "                        DH_NO_data_test = df_DH_NO.iloc[:, :6]\n",
    "                        DH_NO_labels_test = df_DH_NO.iloc[:, 6]\n",
    "                        DH_NO_labels_test.replace('DH', 'NO')\n",
    "                        DH_NO_labels_predict = model.predict(DH_NO_data_test)\n",
    "                        DH_NO_labels_predict[1].replace('DH', 'NO')\n",
    "                        DH_NO_Scores = accuracy_score(DH_NO_labels_test, DH_NO_labels_predict)\n",
    "                        \n",
    "                        df_SL_NO = df[df['class']==('NO' or 'DH')]\n",
    "                        SL_NO_data_test = df_SL_NO.iloc[:, :6]\n",
    "                        SL_NO_labels_test = df_SL_NO.iloc[:, 6]\n",
    "                        SL_NO_labels_test.replace('SL', 'NO')\n",
    "                        SL_NO_labels_predict = model.predict(SL_NO_data_test)\n",
    "                        SL_NO_labels_predict[1].replace('SL', 'NO')\n",
    "                        SL_NO_Scores = accuracy_score(SL_NO_labels_test, SL_NO_labels_predict)\n",
    "                        \n",
    "                        confusion_matrix = metrics.confusion_matrix(labels_test, labels_predict)\n",
    "                        tree_value = model.tree_.value\n",
    "                        decision_path = model.decision_path(data_test).todense()\n",
    "                        if score > max_score:\n",
    "                            max_score = score\n",
    "                            max_test_size = testSize\n",
    "                            max_random_state = state\n",
    "                            max_depth = depth\n",
    "                            max_criterion = cri\n",
    "                            max_confusion_matrix= confusion_matrix\n",
    "                            max_tree_value = tree_value\n",
    "                            max_decision_path = decision_path\n",
    "                            max_model = model\n",
    "                            max_node_count = model.tree_.node_count\n",
    "                            DH_Scores_TMP = DH_Scores\n",
    "                            SL_Scores_TMP = SL_Scores\n",
    "                            NO_Scores_TMP = NO_Scores\n",
    "                            DH_SL_Scores_TMP = DH_SL_Scores\n",
    "                            DH_NO_Scores_TMP = DH_NO_Scores\n",
    "                            SL_NO_Scores_TMP = SL_NO_Scores\n",
    "            max_scores_list.append(max_score)   \n",
    "            max_test_size_list.append(max_test_size)\n",
    "            max_depth_list.append(max_depth)\n",
    "            max_criterion_color = 'b' if max_criterion == 'gini' else 'y'\n",
    "            max_criterion_list.append(max_criterion_color)\n",
    "            max_node_count_list.append(max_node_count)\n",
    "            DH_Scores_list.append(DH_Scores_TMP)\n",
    "            SL_Scores_list.append(SL_Scores_TMP)\n",
    "            NO_Scores_list.append(NO_Scores_TMP)\n",
    "            DH_SL_Scores_list.append(DH_SL_Scores_TMP)\n",
    "            DH_NO_Scores_list.append(DH_NO_Scores_TMP)\n",
    "            SL_NO_Scores_list.append(SL_NO_Scores_TMP)\n",
    "            print('Max TestSize:{} State:{} Depth:{} Criterion: {} model score: {} Node Count: {} \\n DH_Scores: {} SL_Scores: {} NO_Scores: {} \\n DH_SL_Scores: {} DH_NO_Scores: {} SL_NO_Scores: {}'\n",
    "                  .format(max_test_size, max_random_state, max_depth, max_criterion, max_score, max_node_count, DH_Scores_TMP, SL_Scores_TMP, NO_Scores_TMP, DH_SL_Scores_TMP, DH_NO_Scores_TMP, SL_NO_Scores_TMP))\n",
    "#             print('Confusion Matrix: \\n', max_confusion_matrix)\n",
    "#             print('Max Decision Path: ', max_decision_path)\n",
    "#             save_tree_file(max_model, max_test_size, max_random_state, max_depth, \n",
    "#                            max_criterion, max_score, features_set)\n",
    "#     print(len(max_scores_list))\n",
    "#     print(len(max_test_size_list))\n",
    "#     print(len(max_criterion_list))\n",
    "#     visualize_data(max_test_size_list[0::2], max_scores_list[0::2], 'Test Set Size', 'Max Scores', max_criterion_list[0::2], 'Entropy_Graph_With_Test_Set_Size_and_Max_Scores')\n",
    "#     visualize_data(max_test_size_list[1::2], max_scores_list[1::2], 'Test Set Size', 'Max Scores', max_criterion_list[1::2], 'Gini_Graph_With_Test_Set_Size_and_Max_Scores')\n",
    "#     visualize_data(max_test_size_list[0::2], max_depth_list[0::2], 'Test Set Size', 'Max Depth', max_criterion_list[0::2], 'Entropy_Graph_With_Test_Set_Size_and_Max_Depth')\n",
    "#     visualize_data(max_test_size_list[1::2], max_depth_list[1::2], 'Test Set Size', 'Max Depth', max_criterion_list[1::2], 'Gini_Graph_With_Test_Set_Size_and_Max_Depth')\n",
    "#     visualize_data(max_depth_list[0::2], max_scores_list[0::2], 'Max Depth', 'Max Scores', max_criterion_list[0::2], 'Entropy_Graph_With_Max_Depth_and_Max_Scores')\n",
    "#     visualize_data(max_depth_list[1::2], max_scores_list[1::2], 'Max Depth', 'Max Scores', max_criterion_list[1::2], 'Gini_Graph_With_Max_Depth_and_Max_Scores')\n",
    "#     visualize_data(max_node_count_list[0::2], max_scores_list[0::2], 'Node Count', 'Max Scores', max_criterion_list[0::2], 'Entropy_Graph_With_Max_Node_Count_and_Max_Scores')\n",
    "#     visualize_data(max_node_count_list[1::2], max_scores_list[1::2], 'Node Count', 'Max Scores', max_criterion_list[1::2], 'Gini_Graph_With_Max_Node_Count_and_Max_Scores')\n",
    "#     visualize_data(max_test_size_list, DH_Scores_list, 'Test Set Size', 'DH Scores', None, 'Test_Set_Size_and_DH_Scores')\n",
    "#     visualize_data(max_test_size_list, SL_Scores_list, 'Test Set Size', 'SL Scores', None, 'Test_Set_Size_and_SL_Scores')\n",
    "#     visualize_data(max_test_size_list, NO_Scores_list, 'Test Set Size', 'NO Scores', None, 'Test_Set_Size_and_NO_Scores')\n",
    "#     visualize_data(max_test_size_list, DH_SL_Scores_list, 'Test Set Size', 'DH_SL Scores', None, 'Test_Set_Size_and_DH_SL_Scores')\n",
    "#     visualize_data(max_test_size_list, DH_NO_Scores_list, 'Test Set Size', 'DH_NO Scores', None, 'Test_Set_Size_and_DH_NO_Scores')\n",
    "#     visualize_data(max_test_size_list, SL_NO_Scores_list, 'Test Set Size', 'SL_NO Scores', None, 'Test_Set_Size_and_SL_NO_Scores')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "generate_train_split_decision_trees()\n",
    "                    \n",
    "                    \n",
    "                        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "f7e38107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      DH\n",
      "1      DH\n",
      "2      DH\n",
      "3      DH\n",
      "4      DH\n",
      "       ..\n",
      "305    NO\n",
      "306    NO\n",
      "307    NO\n",
      "308    NO\n",
      "309    NO\n",
      "Name: class, Length: 310, dtype: object\n"
     ]
    }
   ],
   "source": [
    "features_set, label_name, training_labels, overall_features_data = generate_data_info(read_data())\n",
    "# print(type(training_labels))\n",
    "# print(type(overall_features_data))\n",
    "# result = pd.concat([overall_features_data, training_labels], axis=1)\n",
    "# result_DH= result[result['class']=='DH']\n",
    "# result_SL= result[result['class']=='SL']\n",
    "training_labels.replace('DH', 'SL')\n",
    "print(training_labels)\n",
    "\n",
    "\n",
    "# print(overall_features_data[:, 0:6] + training_labels[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a00c54e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pelvic_incidence</th>\n",
       "      <th>pelvic_tilt</th>\n",
       "      <th>lumbar_lordosis_angle</th>\n",
       "      <th>sacral_slope</th>\n",
       "      <th>pelvic_radius</th>\n",
       "      <th>degree_spondylolisthesis</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.03</td>\n",
       "      <td>22.55</td>\n",
       "      <td>39.61</td>\n",
       "      <td>40.48</td>\n",
       "      <td>98.67</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>DH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.06</td>\n",
       "      <td>10.06</td>\n",
       "      <td>25.02</td>\n",
       "      <td>29.00</td>\n",
       "      <td>114.41</td>\n",
       "      <td>4.56</td>\n",
       "      <td>DH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.83</td>\n",
       "      <td>22.22</td>\n",
       "      <td>50.09</td>\n",
       "      <td>46.61</td>\n",
       "      <td>105.99</td>\n",
       "      <td>-3.53</td>\n",
       "      <td>DH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.30</td>\n",
       "      <td>24.65</td>\n",
       "      <td>44.31</td>\n",
       "      <td>44.64</td>\n",
       "      <td>101.87</td>\n",
       "      <td>11.21</td>\n",
       "      <td>DH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.71</td>\n",
       "      <td>9.65</td>\n",
       "      <td>28.32</td>\n",
       "      <td>40.06</td>\n",
       "      <td>108.17</td>\n",
       "      <td>7.92</td>\n",
       "      <td>DH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n",
       "0             63.03        22.55                  39.61         40.48   \n",
       "1             39.06        10.06                  25.02         29.00   \n",
       "2             68.83        22.22                  50.09         46.61   \n",
       "3             69.30        24.65                  44.31         44.64   \n",
       "4             49.71         9.65                  28.32         40.06   \n",
       "\n",
       "   pelvic_radius  degree_spondylolisthesis class  \n",
       "0          98.67                     -0.25    DH  \n",
       "1         114.41                      4.56    DH  \n",
       "2         105.99                     -3.53    DH  \n",
       "3         101.87                     11.21    DH  \n",
       "4         108.17                      7.92    DH  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bb9fccf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SL    150\n",
       "NO    100\n",
       "DH     60\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ea1bd151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "def save_tree_file():\n",
    "    dot_data = tree.export_graphviz(model, out_file=None,\n",
    "        feature_names=features_set,\n",
    "        class_names=label_name,\n",
    "        filled=True, rounded=True,\n",
    "        special_characters=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    grpah.format='png'\n",
    "    graph.render('.//Graphs//Graph-{}'.format(i), view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80bfe412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=5)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "# print\n",
    "# training_labels = dftrain.iloc[:,label_column_index]\n",
    "# labels = le.fit_transform(training_labels)\n",
    "clf = tree.DecisionTreeClassifier(max_depth=5)\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "print(clf)\n",
    "print(iris.data)\n",
    "print(iris.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581dea5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iris.pdf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "dot_data = tree.export_graphviz(clf, out_file=None,\n",
    " feature_names=iris.feature_names,\n",
    "class_names=iris.target_names,\n",
    "filled=True, rounded=True,\n",
    " special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render('Iris', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ffd8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277c15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
